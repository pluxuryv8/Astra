# Фаза 0 — Промпт 1: Аудит деградации качества Astra

Дата: 2026-02-22  
Ветка: `phase-0`  
Режим аудита: только текущая архитектура, без внедрения новых фреймворков.

## Область проверки

Проверены узлы, указанные в плане:

- `apps/api/routes/runs.py`
- `core/intent_router.py`
- `core/planner.py`
- `core/brain/router.py`
- `prompts/core_identity.md`
- `prompts/tone_pipeline.md`
- `prompts/variation_rules.md`

Дополнительно просмотрен фактический chat prompt builder, который реально используется из `runs.py`:

- `core/agent.py`

## Топ-10 причин деградации качества (по убыванию влияния)

1. **Перегруженный системный prompt в chat-path (слишком много блоков и оркестраторов).**  
Доказательства: `core/agent.py:1703`, `core/agent.py:1716`, `core/agent.py:1719`, `apps/api/routes/runs.py:516`.  
Почему это бьёт по качеству: в один запрос смешиваются identity, tone pipeline, variation rules, runtime-анализ, pseudo-multi-agent блоки, recall-блоки и reflection-слои. Это размывает приоритет инструкций, увеличивает шанс off-topic и снижает стабильность.

2. **Нет жёсткого лимита длины system prompt по умолчанию.**  
Доказательства: `core/agent.py:1365`, `core/agent.py:1368`, `core/agent.py:1725`.  
Почему это бьёт по качеству: при пустом `ASTRA_CHAT_SYSTEM_PROMPT_MAX_CHARS` prompt не ограничен, что ухудшает latency, провоцирует обрезанные/шумные ответы и нестабильность на локальных моделях.

3. **Агрессивный fast chat path обходит semantic-intent и часть контекстной логики.**  
Доказательства: `apps/api/routes/runs.py:205`, `apps/api/routes/runs.py:221`, `apps/api/routes/runs.py:1114`, `apps/api/routes/runs.py:1173`.  
Почему это бьёт по качеству: короткие, но важные запросы часто идут в упрощённый путь, где пропускаются semantic decision и memory interpreter. В итоге «не понял с первого раза» и слабая персонализация.

4. **При ошибке semantic decision система быстро деградирует в “service fallback” вместо нормального ответа.**  
Доказательства: `apps/api/routes/runs.py:1131`, `apps/api/routes/runs.py:1145`, `apps/api/routes/runs.py:1292`, `apps/api/routes/runs.py:1310`, `core/semantic/decision.py:177`.  
Почему это бьёт по качеству: если semantic JSON невалиден или LLM вернула ошибку, пользователь получает технический fallback, а не полноценный полезный ответ.

5. **Planner по умолчанию скатывается в `COMPUTER_ACTIONS`, если `plan_hint` пуст.**  
Доказательства: `core/planner.py:1100`, `core/planner.py:1105`, `core/planner.py:1106`.  
Почему это бьёт по качеству: неочевидные запросы интерпретируются как действия на компьютере, хотя пользователь ожидал обычный текстовый/аналитический ответ.

6. **Каскад soft-retry делает несколько LLM-вызовов подряд и усиливает нестабильность.**  
Доказательства: `apps/api/routes/runs.py:756`, `apps/api/routes/runs.py:770`, `apps/api/routes/runs.py:790`, `apps/api/routes/runs.py:803`.  
Почему это бьёт по качеству: при одном неудачном ответе запускается цепочка retries/rewrites/base-fallback, что увеличивает время ответа и может ухудшать релевантность из-за “patch over patch”.

7. **Auto web research срабатывает слишком широко и часто.**  
Доказательства: `apps/api/routes/runs.py:836`, `apps/api/routes/runs.py:841`, `apps/api/routes/runs.py:846`, `apps/api/routes/runs.py:1417`.  
Почему это бьёт по качеству: web_search запускается даже при частично нормальном ответе (“uncertain” эвристика), что добавляет лишнюю задержку и повышает риск «грязного» финального текста.

8. **Сборка ответа web research не чистит шум и не ставит строгий quality-gate.**  
Доказательства: `apps/api/routes/runs.py:898`, `apps/api/routes/runs.py:904`, `apps/api/routes/runs.py:906`, `apps/api/routes/runs.py:1054`.  
Почему это бьёт по качеству: результат склеивается почти напрямую из артефакта + источников, поэтому в выдачу попадают мусорные/плохо нормализованные куски.

9. **Выбор модели в brain router завязан на грубые эвристики длины/слов/регексов.**  
Доказательства: `core/brain/router.py:473`, `core/brain/router.py:494`, `core/brain/router.py:511`, `core/brain/router.py:524`.  
Почему это бьёт по качеству: сложность запроса оценивается поверхностно, поэтому запросы нередко попадают не на тот tier модели (fast vs complex), что даёт «хуже чем ожидалось».

10. **Prompt-пакет перегружен и частично конфликтует по правилам стиля.**  
Доказательства: `prompts/core_identity.md:15`, `prompts/core_identity.md:130`, `prompts/tone_pipeline.md:60`, `prompts/tone_pipeline.md:64`, `prompts/variation_rules.md:17`, `prompts/variation_rules.md:107`.  
Почему это бьёт по качеству: одновременно заданы обязательные reflection loops, anti-template banlist и “обязательные” оркестраторы. Для локальных моделей это часто ведёт к многословности и нестабильной подаче.

## План фиксов по файлам (приоритетный)

1. **`apps/api/routes/runs.py` (самый высокий приоритет):**  
Ужесточить `_is_fast_chat_candidate`, не пропускать semantic для пограничных запросов, упростить soft-retry до 1 безопасной попытки, сузить `_should_auto_web_research`, добавить финальный text sanitizer для web/чат ответа.

2. **`core/agent.py` + `prompts/*` (второй приоритет):**  
Сделать компактный runtime prompt: отключить тяжёлые orchestration-блоки по умолчанию для chat, сократить обязательные блоки до минимального ядра, ввести дефолтный лимит `ASTRA_CHAT_SYSTEM_PROMPT_MAX_CHARS`.

3. **`core/planner.py`:**  
Убрать дефолт `KIND_COMPUTER_ACTIONS` при пустом hint; дефолт должен быть безопасным текстовым ответом/уточнением, а не автопилот-действием.

4. **`core/intent_router.py` (+ связка с semantic):**  
Добавить более устойчивый fallback при semantic ошибках: сначала попытка прямого chat-intent с ограниченным policy prompt, а не мгновенный degraded-текст.

5. **`core/brain/router.py`:**  
Перепроверить пороги fast/complex, ввести сигналы из intent/tone вместо только длины, логировать причину выбора модели для анализа регрессий.

6. **`apps/api/routes/runs.py` + tests (гейт качества):**  
Зафиксировать regression-кейсы: off-topic, мусорный web output, потеря контекста, неверная декомпозиция. Без зелёных тестов не переходить к следующей фазе.

## Быстрые победы (можно сделать сразу в следующих промптах)

1. Поставить дефолтный лимит system prompt и укоротить runtime-блоки.  
2. Ослабить fast-path bypass и сузить auto-web-research heuristic.  
3. Добавить чистку “битого текста” перед возвратом финального ответа.

## Вопросы, которые критично влияют на реализацию

1. Что важнее в ближайших фазах: **качество ответа** или **скорость ответа**?  
Влияние: если приоритет качество, держим более строгий routing и quality-gate; если скорость, агрессивнее режем pipeline и отключаем часть проверок.

2. Для web-поиска в chat по умолчанию нужен режим **“чистый ответ без источников”** или **“ответ + источники”**?  
Влияние: это напрямую определяет формат `_compose_web_research_chat_text` и критерии pass/fail в regression тестах.

3. Разрешаем ли по умолчанию тяжёлые режимы (`workflow/conversation/autonomy/dev`) в обычном chat, или включаем их только явным триггером?  
Влияние: это главный рычаг между “живой сверхфункциональностью” и стабильностью/latency в ежедневном использовании.

## Решения владельца (зафиксировано)

1. Приоритет: **качество ответа**, скорость улучшать вторым приоритетом, если возможно без потери качества.
2. Источники в web-ответах: **оставляем**, но оформляем красиво и компактно внизу сообщения (в стиле ChatGPT/Grok).
3. Режимы `workflow/conversation/autonomy/dev`: **автовключаемые**, плюс отдельный ручной переключатель в UI.
